# Problem Statement
We have been tasked with estimating daily cases for two separate business segments aggregated at the country level for the next three months, taking into account the following significant Indian holidays (not a complete list): Diwali, Dussehra, Ganesh Chaturthi, Navratri, Holi, and other Hindu festivals (You are free to use any publicly available open source external datasets). Other instances may include:

Weather Macroeconomic variables Note that the external dataset must belong to a reliable source.

Weather and macroeconomic factors It's worth noting that the external dataset must come from a reputable source.

Dictionary of Data The train information was delivered in the following format:

Historical data for business sector 1 has been made accessible at the branch ID level. Historical data for business sector 2 has been made accessible at the state level.
File Variable Training application date is a term that is defined as follows: Segment of application date 1/2 of the business segment branch id An anonymous identifier for the branch where the application was received The state where the application was submitted (Karnataka, MP etc.) state where the application was received (Central, East etc.) case count is a function that counts the number of cases (Target) The number of cases/applications that have been received

Forecasting for the dates supplied in the test set for each section must be done at the nation level.

id of a variable definition Application date for each sample in the test set with a unique id Segment of application date 1/2 of the business segment

### Evaluation
Metrics for Evaluation *MAPE (Mean Absolute Percentage Error) M is the assessment metric for rating the forecasts, following the formula:

Where At denotes the current value and Ft denotes the predicted value.

For both portions, the final score is derived using MAPE and the formula:

### Important Notes
When deciding on the winners, the practicality of implementing the best ideas will be taken into account. Both business groups must be satisfied with the outcome of the solution.

The data from the public and private split tests is separated into two categories: public (1st month) and private (2nd month) (Next 2 months)
```{r}
library(readr)
library(dplyr)
library(lubridate) # Working with Dates
library(reshape)
library(reshape2)# Reshaping the data
library(tidyr)
library(tidyverse)
library(RQuantLib)
library(bizdays)
library(timeDate)

## Visualizations
library(ggplot2)
library(plotly)

### ARCH ###
library(prophet)

## Model Building and Forecasting
library(timeSeries)
library(fpp2)
library(forecast)
```



## Reading Data Set

```{r}
dat_train = data.frame(read.csv("D:/dataset/train.csv"))
dat_test = data.frame(read.csv("D:/dataset/test.csv"))
dat_test$application_date<-as.Date(dat_test$application_date,format = "%d-%m-%Y")
```

## Checking Structure and Summary of the data 

```{r}
str(dat_train)
summary(dat_train)
```

## Checking percentage of missing values across different columns 

```{r}
colSums(is.na(dat_train))/nrow(dat_train)*100
```

## Variable conversion
#### Converting applicate date into relevant date format.

```{r}
dat_train$application_date<-as.Date(dat_train$application_date,format = "%d-%m-%Y")
dat_train$segment<-as.character(dat_train$segment)
str(dat_train)
```

## Sorting the dataset as per Date and Segment
```{r}
dat_train<-dat_train[order(dat_train$segment,dat_train$application_date),]
```

## Creating month variable

```{r}
dat_train$yr<-year(dat_train$application_date)
dat_train$mnth<-month(dat_train$application_date)
dat_train$qrtr<-quarters(dat_train$application_date)
dat_train$yr_mnth<-factor(paste(dat_train$yr,dat_train$mnth,sep = ""))
dat_train$yr_quarter<-factor(paste(dat_train$yr,dat_train$qrtr,sep ="-"))
dat_train$week<-week(dat_train$application_date)
```

## Visualization 
### Dataset

```{r}
library(MASS)
mdl_dt<-dat_train[,c("segment","application_date","no_of_applicants")]
```

## Number of cases/Applications 
### Daily trend
```{r}
cs_trend<-mdl_dt%>%group_by(segment,application_date)%>%summarise(No_cases = mean(no_of_applicants))
```

## Plot - Daily Trend

```{r}
ggplot(cs_trend,aes(x = application_date,y = No_cases,color = segment))+geom_line(stat = "identity")+labs(title = "Number of cases")+scale_x_date(date_labels = "%b-%Y")+facet_grid(segment~.,scale = "free")
```


### Number of cases from APR'2017 to JUL'2019

```{r}
fin_cs<-mdl_dt %>% group_by(segment,application_date) %>% summarise(No_cases = mean(no_of_applicants))
```

## Visualisation for checking seasonality 

### Segmenting data
```{r}
seg_1<-subset(fin_cs,fin_cs$segment == 1)
seg_1$segment<-NULL

seg_2<-subset(fin_cs,fin_cs$segment == 2)
seg_2$segment<-NULL
```


## Train and Test Split
### Segment 1
#### Converting to xts object
```{r}
seg1ts<- ts(seg_1[,2], frequency=365, start=c(2017,90),end = c(2019,165))
seg1ts_tr<-ts(seg1ts[1:(0.75*nrow(seg1ts))])
seg1ts_ts<-ts(seg1ts[(0.75*nrow(seg1ts)):nrow(seg1ts)])
```


### Segment 2
#### Converting to xts object
```{r}
library(xts)
seg2ts<- ts(seg_2[,2], frequency=365, start=c(2017,90),end = c(2019,203))
seg2ts_tr<-ts(seg2ts[1:(0.75*nrow(seg2ts))])
seg2ts_ts<-ts(seg2ts[(0.75*nrow(seg2ts)):nrow(seg2ts)])
```


```{r}
seg2<-xts(seg_2[,-1],order.by = as.POSIXct(strptime(seg_2$application_date, "%Y-%m-%d")))
seg2.train<-window(seg2,end = "2018-12-31")
seg2.test<-window(seg2,start = "2019-01-01")
```



## Model Building 
### Segment 2 
#### Holtwinters Triple Exponential Smoothing
#### Decomposition of Time Series ##
```{r}
decmp_seg2<-decompose(seg2ts,type = "additive")
autoplot(decmp_seg2)
```


## Holtwinters Double Exponential Smoothing
```{r}
fit_ets<-ets(seg2.train,model = "AAN")
fit_ets
ft<-forecast.ets(fit_ets,h = 204)

accuracy(ft,seg2.test)

```


# Optimum value of Beta
```{r}
beta <- seq(0.001, 0.05, 0.001)
RMSE<-NA
for(i in seq_along(beta)) {
  hw.expo <- ets(seg2.train, model = "AAN", alpha = 0.9919,beta = beta[i])
  future <- forecast.ets(hw.expo, h = 204)
  RMSE[i] = accuracy(future, seg2.test)[2,2]
}

error <- data_frame(beta, RMSE)
minimum <- filter(error, RMSE == min(RMSE))
ggplotly(ggplot(error, aes(beta, RMSE)) +
           geom_line() +
           ggtitle("beta's impact on forecast errors"))
```


## Building model with different alpha value
```{r}
fit_hw<-ets(seg2.train,model = "AAN",alpha = 0.8919,beta = 0.0008)
summary(fit_hw)
checkresiduals(fit_hw)

```
```{r}

future<-forecast.ets(fit_hw,h = 204)
```



### Model Accuracy 
```{r}
accuracy(future,seg2.test)
seg2.test[1:20]
```

####Segment 1 Model

```{r}
library(prophet)
```


################## Final Prediction #################
# Segment 1 #

```{r}
seg1<-data.frame(seg_1)
names(seg1)<-c("ds","y")

dat_ts1<-dat_test[dat_test$segment == 1,2:3]
names(dat_ts1)<-c("ds","y")

prp_mdl1<-prophet(seg1,seasonality.mode = "multiplicative")
forecst1 = data.frame(predict(prp_mdl1,dat_ts1))

dat_ts3<-dat_test[dat_test$segment == 1,]
dat_ts3<-cbind(dat_ts3,no_of_applicants = round(forecst1$yhat))
names(dat_ts3)<-c("id","application_date","segment","no_of_applicants")
```


# Segment 2 #

```{r}
fit_hlw<-ets(seg2.train,model = "AAN",alpha = 0.9919,beta = 0.0004)
summary(fit_hlw)

forcst_val<-data.frame(forecast.ets(fit_hlw,h = 234,alpha = 30))
nrow(forcst_val)
no_of_applicants<-data.frame(forcst_val$Point.Forecast[205:234])

dat_ts<-dat_test[dat_test$segment == 2,]
dat_ts<-cbind(dat_ts,no_of_applicants = round(no_of_applicants))
names(dat_ts)<-c("id","application_date","segment","no_of_applicants")

dat_t<-rbind(dat_ts3,dat_ts)
write.csv(dat_t,"D:/sub9.csv")
```

